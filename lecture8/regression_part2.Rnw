\documentclass[10pt]{article}
\usepackage{amssymb, amsmath, amsthm, verbatim, layout, setspace, graphicx, float, geometry}
\usepackage{booktabs}

\newcommand{\textnote}[1]{\textbf{\textit{#1}}}
\newcommand{\code}[1]{\textbf{\textit{#1}}}
\newcommand{\grad}{\nabla}

% ------ layout ------

\begin{document}
\title{Linear Regression}
\author{Jeremy Oldfather}
\maketitle

<<setup,echo=FALSE,message=FALSE>>=
opts_chunk$set(fig.align="center",size="small")
library(ggplot2)
library(readr)
library(dplyr)
@

\section*{Review}

\subsection*{What is linear regression?}

Start with a line.

\begin{align}
y &= mx + b \label{line}
\end{align}


We know how this appears visually. The line crosses the y-axis at the height of $b$ (intercept) and for each unit we move left or right, we move up or down by $m$ (slope) units. For example, with $b=10$ and $m=.7$, we get the following line:

<<simple_line,fig.height=3,fig.width=6,dev='pdf'>>=
b<-10
m<-.7
x<-0:50
y<-m*x + b
qplot(x,y,geom="line")
@

Since the line has two parameters, $m$ and $b$, we could take any {\bf two} distinct points, draw a line between them, and solve for the parameters. If we only have {\bf one} point, we could draw infinitely many lines through the point.

Let's take this idea to some data. I have some data on the sales of DC properties. We can conceptualize each property as a point. Here is the \code{sale\_price} and \code{living\_sqft} of the first two properties. 

<<load_data,warning=FALSE,message=FALSE,cache=TRUE>>=
# see: https://github.com/theoldfather/DC_Properties
sales<-read_csv("~/Projects/DC_Properties/data/sales_clean.csv")
features<-read_csv("~/Projects/DC_Properties/data/features_clean.csv")
details<-read_csv("~/Projects/DC_Properties/data/details_clean.csv")
@
<<get_sales>>=
d<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  select(sale_price,living_sqft) 
head(d,2) %>% data.frame()
@

Plotting these two points, we get the following scatterplot.

<<plot2,fig.height=3,dev='pdf'>>=
ggplot(d[1:2,],aes(x=living_sqft,y=sale_price)) + geom_point()
@

We could draw a segment between the lines by changing \code{geom\_point()} to \code{geom\_line()}, but let's do it by finding the slope and intercept, and then plotting the line through the points.

If we subscript equation \ref{line} with $i$ to represent each observation, we can write it in a condensed notation as

\begin{align}
y_i &= m x_i + b \label{line_general}
\end{align}

and then expand it for $i \in \{1,2\}$.

\begin{align}
y_1 &= m x_1 + b \\
y_2 &= m x_2 + b
\end{align}

Solving for $m$ and $b$ we get

\begin{align}
m = \frac{y_2 - y_1}{x_2-x_1} \label{solution_slope} \\
b = y_1 - mx_1 \label{solution_intercept}
\end{align}

<<fit_line,fig.height=3,dev='pdf'>>=
fit_line<-function(y,x){
  m <- (y[2]-y[1])/(x[2]-x[1])
  b <- y[1] - m*x[1]
  return(c(b,m))
}
beta<-fit_line(d$sale_price[1:2],d$living_sqft[1:2])
ggplot(d[1:2,],aes(x=living_sqft,y=sale_price)) + 
  geom_point() + geom_abline(intercept=beta[1],slope=beta[2])
@

Do you believe this is an accurate model of the relationship between sales price and the living area of a home? $b=\Sexpr{ beta[1] }$ implies that a house with no space would be worth \$3 million. And $m=\Sexpr{ beta[2] }$ implies that increasing the living area of a home by 1 sqft would decrease its values by \$2,008. 

Thankfully, we have data on more than two homes. But what does it mean to draw a line through {\bf more than two} points? We know our model above is very bad---so not every point will fall on the same line.

Here is the rest of our data plotted on log-log scale (we will talk about log-log scale later).

<<scatter_all,fig.height=3,warning=FALSE,dev='pdf'>>=
ggplot(d,aes(x=living_sqft,y=sale_price)) + 
  geom_point(alpha=.1) + scale_y_log10() + scale_x_log10()
@

This scatterplot seems promising--it visually re-enforces our beliefs that people place more value on larger homes. But we still want to quantify these beliefs to inform decision about buying, selling, making home improvements, etc.

Even though there is not a single line that can pass through each point, we can still find an "average line" that best-fits the data. This is the essence of regression.

\subsection*{Simple Linear Regression}

Let's expand our general formula for a line above (Equation \ref{line_general}) to express the fact that our line does not fit exactly, but rather has some slack distance between it and each point. 

\begin{align}
y_i &= mx_i + b + \epsilon_i
\end{align}

The slack or {\bf residual } term, $\epsilon_i$, also gets a subscript $i$ because we cannot assume that the line will be the same distance (vertically) from every point. Or in other words, our model will explain some home prices better than others. 

Let's also formalize the notation a bit more and replace $b$ with $\beta_0$ and $m$ with $\beta_1$. Many papers use beta ($\beta$) to represent the parameters of the model, but sometimes you will also find the intercept represented as alpha ($\alpha$). This is simply different notation and has the same meaning. 

\begin{align}
y_i &= \beta_0 + \beta_1 x_i + \epsilon_i
\end{align}

Next, we need some way of defining what it means to be the line of best-fit. What we want conceptually is the model that explains our data most accurately, and on the flip-side, the model that is the least inaccurate. Let's use the latter conceptualization and rewrite the equation above in terms of $\epsilon_i$

\begin{align}
\epsilon_i &=  ( y_i -  (\beta_0 + \beta_1 x_i) )
\end{align}

But minimizing a single residual is not helpful. Residuals can be negative, so the further we shift the fitted line above our points, the more and more negative each residual will become. We could keep shifting the line higher and higher and always produce a more negative residual. What if we square the residual---$\epsilon^2_i$? Now a point that is 6 units below the fitted line will be as equally bad as a point 6 units above the line.  

\begin{align}
\epsilon^2_i &=  ( y_i -  (\beta_0 + \beta_1 x_i) )^2
\end{align}

And we want to consider the value of all the squared residuals as once so that each point plays a role is how our line is fit. Summing over the squared residuals leads to the following formulation:

\begin{align}
\sum_i \epsilon^2_i &=  \sum_i ( y_i -  (\beta_0 + \beta_1 x_i) )^2
\end{align}

This is the intuition behind the loss function (the function to be minimized) in simple linear regression called the \textbf{Sum of Squared Residuals (SSR)}.

\begin{align}
SSR(\beta_0, \beta_1) &= \sum_i ( y_i -  (\beta_0 + \beta_1 x_i) )^2
\end{align}

By minimizing the SSR, we can find unique solutions for the coefficients $\beta_0$ and $\beta_1$. One way to do this is by taking the derivative of $SSR(\beta_0, \beta_1)$ with respect to each $\beta$, setting the derivatives equal to zero, and then solving for the coefficients. The following solutions are exactly analogous to the solutions for the intercept and slope of the line in Equations (\ref{solution_intercept}) and (\ref{solution_slope}).

\begin{align}
\beta_1 &= \frac{ \sum_i y_i x_i - \frac{\sum_i y_i \sum_i x_i}{n} }{ \sum_i x_i^2 - \frac{(\sum_i x_i)^2}{n} } \\
\beta_0 &=  \sum_i y_i - \beta_1 \sum_i x_i 
\end{align}

This might look messy at first, but notice there are really only 4 statistics we need to compute: 

\begin{align}
\sum_i & y_i x_i \\
\sum_i & y_i \\
\sum_i & x_i \\
\sum_i & x_i^2
\end{align}

Using this idea, let's write a function to estimate the betas in R. 

<<def_fit_simple_lm>>=
fit_simple_lm<-function(y,x){
  n<-length(y)
  syx<-sum(y*x)
  sy<-sum(y)
  sx<-sum(x)
  sxx<-sum(x^2)
  b1<- (syx - sy*sx/n)/(sxx - sx^2/n)
  b0<- (sy - b1*sx)/n
  return(c(b0,b1))
}
@

Now we can use our function to estimate the betas--the coefficients of our simple linear model.

<<use_fit_simple_lm,fig.height=4,dev='pdf'>>=
# find our betas
beta<-fit_simple_lm(y=d$sale_price,x=d$living_sqft)
# use betas to create a data frame of fitted sales prices 
E<-data.frame(sale_price=beta[1]+beta[2]*d$living_sqft,
                   living_sqft=d$living_sqft)
# plot our data and the fitted line
ggplot(d,aes(x=living_sqft,y=sale_price)) + 
  geom_point(alpha=.1) + 
  geom_line(data=E,colour="orange") + 
  scale_y_log10() + scale_x_log10() 
@

The fitted line is beginning to look more intuitive and it is certainly a better model than the one constructed from two points. But why is it curved? I thought we fitted a "line"?

The answer is that the order in which we apply convex/concave functions to our data and fitted model matters! We can get this right by referencing \textbf{Jensen's Inequality}, which states that when we apply a concave function (think of a hill that could "cave" in) to an expected value, its output will always be greater than or equal to the expected values of the same function applied to the input. If $f(x)$ is the concave function, the inequality is:

\begin{align}
f(E(x)) & \ge E(f(x))
\end{align}

In our case, $log(x)$ is the concave function and $E(x)$ is our fitted line, $\hat{y}_i = \beta_0 + \beta_1 x_i$. We can visualize the difference this order of operations makes by plotting both alternatives. The \textbf{orange} line is what we plotted before, $log(E(\beta_0 + \beta_1 x_i))$, and the \textbf{blue} one is the model fitted on the log data, $E(log(\beta_0 + \beta_1 x_i))$.

<<fit_simple_lm_log,fig.height=4,cache=FALSE,dev='pdf'>>=
# log scale both y and x manually
logd <- d %>% 
  mutate(log_sale_price=log(sale_price),
         log_living_sqft=log(living_sqft))
# estimate betas again based on log-log model
beta<-fit_simple_lm(y=logd$log_sale_price,x=logd$log_living_sqft)
# E(log(x))
E_log<-data.frame(log_sale_price=beta[1]+beta[2]*logd$log_living_sqft,
                   log_living_sqft=logd$log_living_sqft)
# log(E(x))
log_E<-E %>% mutate(log_sale_price = log(sale_price),
                    log_living_sqft = log(living_sqft))
# plot the log-log data manually (no scale_y_log10() or scale_x_log10() )
ggplot(logd,aes(x=log_living_sqft,y=log_sale_price)) + 
  geom_point(alpha=.1) + 
  geom_line(data=log_E,colour="orange") +
  geom_line(data=E_log,colour="blue") 
beta
@

\subsection*{Interpretting a Linear Regression}

Now that we finally have the fitted line that we expected to see (the blue one), let's step back and think about how we should interpret the coefficients that parameterize it: $\beta_1 =$ \Sexpr{beta[2]} and $\beta_0 =$ \Sexpr{beta[1]}.

\subsubsection*{level-level}

With a level-level model, we have the raw levels of variables on each side of the regression.

$$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$

Since this model has the form of a line, we can interpret $\beta_1$ as the slope, $\Delta y / \Delta x$. To prove this to ourselves, we only need to take the derivative of the model with respect to $x$.

\begin{align}
\frac{dy}{dx} &=\beta_1 \\
\beta_1 &= \frac{dy}{dx}
\end{align}

So we can interpret $\beta1$ as the {\bf marginal effect} since a 1 unit increase in $x$ leads to a $\beta_1$ unit change in $y$.

\subsubsection*{log-level}

A log-level model is not as intuitive, but we can apply the same analysis to arrive at an interpretation.

\begin{align}
log(y_i) &=  \beta_0 + \beta_1 x_i + \epsilon_i \\
y_i &= e^{\beta_0 + \beta_1 x_i + \epsilon_i}
\end{align}

Taking the derivative with respect to $x$:

\begin{align}
\frac{dy}{dx} &= \beta_1 e^{\beta_0 + \beta_1 x_i + \epsilon_i} \\
 &= \beta_1 y 
\end{align}

Solving for $\beta_1$:

\begin{align}
 \beta_1 &= \frac{dy}{dx} \frac{1}{y}
\end{align}

The \textbf{marginal effect}, $dy / dx = \beta_1 y$, implies that a change in $x$ will impact $y$ differently depending on the level of $y$. For instance, if $\beta_1 = .001$, and the current value of my home is \$100,000 and I decide to build an extension that increases its area by 200 sqft., then I would expect its change in value from the improvement to be $$.001 \cdot 100,000 \cdot 200 = 20,000$$ But if my home is currently worth \$200,000, then I expect the improvement to increase it by $$.001 \cdot 200,000 \cdot 200 = 40,000$$

\subsubsection*{log-log}

For a log-log model (our example for this lession), we can apply the same analysis as the previous two models. Starting from the model equation:

\begin{align}
log(y_i) &=  \beta_0 + \beta_1 log(x_i) + \epsilon_i \\
y_i &= e^{\beta_0 + \beta_1 log(x_i) + \epsilon_i}
\end{align}

Take the derivative with respect to $x$:

\begin{align}
\frac{dy}{dx} &= \beta_1 e^{\beta_0 + \beta_1 log(x_i) + \epsilon_i} \cdot \frac{1}{x_i} \\
 &= \beta_1 \frac{y}{x} 
\end{align}

Solving for $\beta_1$:

\begin{align}
 \beta_1 &= \frac{dy}{dx} \frac{x}{y}
\end{align}

The \textbf{marginal effect}, $dy / dx = \beta_1 y / x$, implies that a change in $x$ will impact $y$ differently depending on the level of $y$ relative to $x$. For instance, let's say $\beta_1 = .001$, the current value of my home is \$100,000, and its living area is 1000 sqft. Now if I decide to build an extension that increases its area by 200 sqft., then I would expect its change in value from the improvement to be 

$$.001 \cdot 100,000 / 1000 \cdot 200 = 20$$ 

But if my home is currently worth \$200,000, then I expect the improvement to increase its value by 

$$.001 \cdot 200,000/1000 \cdot 200 = 40$$

Since the model we have been using is in log-log form, let's plug its coefficient for living area into my hypothetical home renovation scenerio.

$$0.7063513 \cdot 200,000/1000 \cdot 200 = 14,127.03$$


\subsection*{Simple Linear Regression in R }

Thankfully, R has some built-in regression tools--we don't need to write our own each time we want to fit a model. All we need is a data frame and model formula. First, let's create the data frame.

<<data_model>>=
# subset some data for a regression
# and make sure both sale_price and living_sqft are log-scaled
data.model<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  left_join(select(details,-sale_price,-neighborhood),by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  mutate(log_sale_price=log(sale_price),
         log_living_sqft=log(living_sqft))

@

Next, let's write our model as an R formula. Our model is:

$$ log(sale\_price_i) = \beta_0 + \beta_1 log(living\_sqft_i) + \epsilon_i$$

In R, we simply write

$$ log\_sale\_price \sim log\_living\_sqft $$

Then we give both pieces of information to {\bf lm()}, R's linear model function.

<<fit_lm_a>>=
lm(log_sale_price ~ log_living_sqft, data=data.model)
@

Notice that in the formula, we do not need to specify the implicit {\bf 1} for $\beta_0$. R assumes we want an intercept by default. But we could specify this if we really want a reminder that it is there.

<<fit_lm_b>>=
lm(log_sale_price ~ 1 + log_living_sqft, data=data.model)
@

And if we wanted a model without an intercept, we could simply subtract {\bf 1} from the formula. This forces our fitted line to be drawn through the origin.

<<fit_lm_c>>=
lm(log_sale_price ~ -1 + log_living_sqft, data=data.model)
@

\subsubsection*{Summarizing the regression}

The {\bf lm()} function returns an object that contains information about the regression that was run. If we do not assign the object to a variable, it will print the estimated coefficients to the screen. However, it is better to keep our results so we can summarize the regression in a paper, use the estimated coefficients for prediction, or analyze the residuals.

So let's run the previous regression and assign to the variable {\bf fit}.

<<fit_lm>>=
fit<-lm(log_sale_price ~ log_living_sqft, data=data.model)
@

The fitted lm object contains too much information to print here, but you can inspect it with the {\bf str()} function or by clicking on it in your Environment panel in Rstudio. The {\bf summary()} function will print a prettier version of the results to screen, similar to the output in Stata.

<<summary_fit>>=
summary(fit)
@

And to get latex output of the table, we can use the { \bf stargazer } and print the results straight into our PDF.

<<latex_fit,results='asis',message=FALSE>>=
library(stargazer) # you will need to install this package
stargazer(fit,type="latex",style="qje")
@


\subsubsection*{Extracting Model Components}

As we have seen, the fit object returned by {\bf lm()} contains the information about the regression that created the object. A benefit of holding on to the object is that we can extract components from the model and analyze them and/or compare them to those from other models.

\textbf{Coefficients ($\beta_k$).}  The betas of the model are stored as a named vector in the fit object and can be extracted by either explicitly referencing them,

<<extract_betas1>>=
fit$coefficients
@

or by applying the {\bf coef()} function to the object.

<<extract_betas2>>=
coef(fit)
@

\textbf{Fitted Values ($\hat{y}$).}  The fitted values, or simply $\hat{y}$, can be extracted by applying the {\bf fitted()} function to the object. 

<<extract_fitted,fig.height=3,dev='pdf'>>=
rbind(data.frame(y=data.model$log_sale_price,x=data.model$log_living_sqft,label="actual"),
      data.frame(y=fitted(fit),x=data.model$log_living_sqft,label="fitted")
      ) %>%
  ggplot(aes(x,y,group=label,colour=label)) + geom_point()
@

From our model, $\hat{y}_i = \beta_0 + \beta_1 x_i$ and $\epsilon_i = y_i - \hat{y}_i$. So in the scatterplot above, every $\hat{y}_i$ has a corresponding $y_i$ either directly above or below it. And the residual $\epsilon_i$ is the difference between them vertically. We can check this:

<<check_fitted>>=
all.equal(resid(fit),(data.model$log_sale_price - fitted(fit)))
@

\textbf{Residuals ($\epsilon_i$).}  In you missed it in the last step, we can also extract the residuals, $\epsilon_i$, by applying the {\bf resid()} function to the fit object.

<<summarize_residuals,fig.height=3>>=
resid(fit) %>% summary()
@

Residuals play a key role in our analysis of the goodness-of-fit of our model and will provide motivation for introducing {\bf multiple linear regression} in the next section.

\subsection*{Multiple Linear Regression}

When we regress $y$ on more than one independent variable, say ($x_1, x_2,...,x_k$) rather than simply ($x_1$), the regression framework is referred to as {\bf multiple linear regression}.

What might motivate us to introduce multiple regressors? One reason might be that we have reason to suspect {\bf omitted variable bias}. In order for our model to be subject to omitted variable bias, we must have both of the following problems:

\begin{description}
  \item [$E(y|x_1,x_2) \ne E(y|x_1) $] :  Meaning our expectation about the dependent variable $y$ changes when we include the additional independent variable. If $E(y|x_1,x_2) = E(y|x_1)$, we would say that $y$ is conditionally independent of $x_2$---meaning either $x_2$ is completely irrevelant to predicting $y$, or that any information contained in $x_2$ is already contained in $x_1$.
  \item [$Cor(x_1, x_2) \ne 0 $] :  Meaning the additional variable $x_2$ must be correlated with a current dependent variable $x_1$.
\end{description}

For example, when we plot the distribution of residuals from our previous model by neighborhood, we can see that the variable {\bf neighborhood} meets the first critera for omitted variable bias since for Anacostia we would consistently overstate the impact of a change in living area on the change in sales price.

<<boxplot_residuals,fig.height=3,dev='pdf'>>=
selected.nhoods<-c("FOGGY BOTTOM","ANACOSTIA","LEDROIT PARK","TAKOMA","GEORGETOWN")
data.frame(e=resid(fit),g=data.model$neighborhood) %>%
  filter(g %in% selected.nhoods) %>%
  ggplot(aes(y=e,x=g)) + geom_boxplot()
@

To check the second criteria, that $Cor(living\_area,neighborhood) \ne 0$, we can plug a dummy variable for each neighborhood into R's {\bf cor()} function.

<<dummy_cor>>=
cor(data.model$log_living_sqft,data.model$neighborhood=="ANACOSTIA")
cor(data.model$log_living_sqft,data.model$neighborhood=="FOGGY BOTTOM")
cor(data.model$log_living_sqft,data.model$neighborhood=="GEORGETOWN")
cor(data.model$log_living_sqft,data.model$neighborhood=="LEDROIT PARK")
cor(data.model$log_living_sqft,data.model$neighborhood=="TAKOMA")
@

Anacostia, Ledroit Park, and Takoma are either weakly or not all correlated with living area, but Foggy Bottom is negatively correlated and Georgetown is weakly positively correlated.

To see the connection between including these neighborhoods in the regression, I will create a new variable that sets all other neighborhoods to "other". I put "aa" at the beginning because R treats the first element of a factor variable as the base value for the regression. So by doing this, we can see how the neighborhoods we analyzed above affect our regression in terms of all other DC neighborhoods.

<<fit_5_nhoods>>=
fit2<-data.model %>% 
  mutate(nhoods5=ifelse(neighborhood %in% selected.nhoods,
                        neighborhood,
                        "aa_other")
         ) %>%
  lm(log_sale_price ~ log_living_sqft + factor(nhoods5),data=.)
summary(fit2)
@

Comparing the coefficients for each neighborhood with the distribution of their residuals in the boxplot above, we can see the pattern. The coefficient for a dummy shifts our fitted line up or down by the amount of the coefficient.  For example, when all the dummies are turned off (equal zero), 

\begin{align*}
 \hat{y}_i &= 7.913192 + 0.699649 x_i
\end{align*}

But when Anacostia equals 1,

\begin{align*}
\hat{y}_i &= 7.913192 + 0.699649 x_i - 0.832172(1) \\
  &= 7.08102 + 0.699649 x_i
\end{align*}

Now I will add dummies for the rest of the DC neighborhoods and run the regression again.

<<fit_all_nhoods>>=
fit3<-lm(log_sale_price ~ log_living_sqft + factor(neighborhood),data=data.model)
coef(fit3)[1:2]
summary(resid(fit3))
@


\newpage

% ----- start appendices -----

\include{appendix}

\end{document}