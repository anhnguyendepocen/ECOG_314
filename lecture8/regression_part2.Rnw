\documentclass[10pt]{article}
\usepackage{amssymb, amsmath, amsthm, verbatim, layout, setspace, graphicx, float, geometry}
\usepackage{tabularx}
\usepackage{booktabs}

\newcommand{\textnote}[1]{\textbf{\textit{#1}}}
\newcommand{\code}[1]{\textbf{\textit{#1}}}
\newcommand{\grad}{\nabla}
\newcommand\inv[1]{#1\raisebox{1.15ex}{$\scriptscriptstyle-\!1$}}

% ------ layout ------

\begin{document}
\title{Linear Regression: Part 2}
\author{Jeremy Oldfather}
\maketitle

<<setup,echo=FALSE,message=FALSE>>=
opts_chunk$set(fig.align="center",size="small")
library(ggplot2)
library(readr)
library(dplyr)
library(magrittr)
@


<<catchup,warning=FALSE,message=FALSE,cache=TRUE,echo=FALSE>>=
# see: https://github.com/theoldfather/DC_Properties
sales<-read_csv("~/Projects/DC_Properties/data/sales_clean.csv")
features<-read_csv("~/Projects/DC_Properties/data/features_clean.csv")
details<-read_csv("~/Projects/DC_Properties/data/details_clean.csv")

d<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  select(sale_price,living_sqft) 

fit_line<-function(y,x){
  m <- (y[2]-y[1])/(x[2]-x[1])
  b <- y[1] - m*x[1]
  return(c(b,m))
}

fit_simple_lm<-function(y,x){
  n<-length(y)
  syx<-sum(y*x)
  sy<-sum(y)
  sx<-sum(x)
  sxx<-sum(x^2)
  b1<- (syx - sy*sx/n)/(sxx - sx^2/n)
  b0<- (sy - b1*sx)/n
  return(c(b0,b1))
}

# subset some data for a regression
# and make sure both sale_price and living_sqft are log-scaled
data.model<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  left_join(select(details,-sale_price,-neighborhood),by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  mutate(log_sale_price=log(sale_price),
         log_living_sqft=log(living_sqft))

fit<-lm(log_sale_price ~ log_living_sqft, data=data.model)

@


\subsection*{Multiple Linear Regression - Last Time }

When we regress $y$ on more than one independent variable, say ($x_1, x_2,...,x_k$) rather than simply ($x_1$), the regression framework is referred to as {\bf multiple linear regression}.

What might motivate us to introduce multiple regressors? One reason might be that we have reason to suspect {\bf omitted variable bias}. In order for our model to be subject to omitted variable bias, we must have both of the following problems:

\begin{description}
  \item [$E(y|x_1,x_2) \ne E(y|x_1) $] :  Meaning our expectation about the dependent variable $y$ changes when we include the additional independent variable. If $E(y|x_1,x_2) = E(y|x_1)$, we would say that $y$ is conditionally independent of $x_2$---meaning either $x_2$ is completely irrevelant to predicting $y$, or that any information contained in $x_2$ is already contained in $x_1$.
  \item [$Cor(x_1, x_2) \ne 0 $] :  Meaning the additional variable $x_2$ must be correlated with a current dependent variable $x_1$.
\end{description}

For example, when we plot the distribution of residuals from our previous model by neighborhood, we can see that the variable {\bf neighborhood} meets the first critera for omitted variable bias since for Anacostia we would consistently overstate the impact of a change in living area on the change in sales price.

<<boxplot_residuals,fig.height=3,dev='pdf'>>=
selected.nhoods<-c("FOGGY BOTTOM","ANACOSTIA","LEDROIT PARK","TAKOMA","GEORGETOWN")
data.frame(e=resid(fit),g=data.model$neighborhood) %>%
  filter(g %in% selected.nhoods) %>%
  ggplot(aes(y=e,x=g)) + geom_boxplot()
@

To check the second criteria, that $Cor(living\_area,neighborhood) \ne 0$, we can plug a dummy variable for each neighborhood into R's {\bf cor()} function.

<<dummy_cor>>=
cor(data.model$log_living_sqft,data.model$neighborhood=="ANACOSTIA")
cor(data.model$log_living_sqft,data.model$neighborhood=="FOGGY BOTTOM")
cor(data.model$log_living_sqft,data.model$neighborhood=="GEORGETOWN")
cor(data.model$log_living_sqft,data.model$neighborhood=="LEDROIT PARK")
cor(data.model$log_living_sqft,data.model$neighborhood=="TAKOMA")
@

Anacostia, Ledroit Park, and Takoma are either weakly or not at all correlated with living area, but Foggy Bottom is negatively correlated and Georgetown is weakly positively correlated.

To see the connection between including these neighborhoods in the regression, I will create a new variable that sets all other neighborhoods to "other". I put "aa" at the beginning because R treats the first element of a factor variable as the base value for the regression. So by doing this, we can see how the neighborhoods we analyzed above affect our regression in terms of all other DC neighborhoods.

<<fit_5_nhoods>>=
fit2<-data.model %>% 
  mutate(nhoods5=ifelse(neighborhood %in% selected.nhoods,
                        neighborhood,
                        "aa_other")
         ) %>%
  lm(log_sale_price ~ log_living_sqft + factor(nhoods5),data=.)
summary(fit2)
@

Comparing the coefficients for each neighborhood with the distribution of their residuals in the boxplot above, we can see the pattern. The coefficient for a dummy shifts our fitted line up or down by the amount of the coefficient.  For example, when all the dummies are turned off (equal zero), 

\begin{align*}
 \hat{y}_i &= 7.913192 + 0.699649 x_i
\end{align*}

But when Anacostia equals 1,

\begin{align*}
\hat{y}_i &= 7.913192 + 0.699649 x_i - 0.832172(1) \\
  &= 7.08102 + 0.699649 x_i
\end{align*}

Now I will add dummies for the rest of the DC neighborhoods and run the regression again.

<<fit_all_nhoods>>=
fit3<-lm(log_sale_price ~ log_living_sqft + factor(neighborhood),data=data.model)
coef(fit3)[1:2]
summary(resid(fit3))
@

\subsection*{Multiple Linear Regression - Continuation }

Previously, we showed how to fit a line using simple linear regression (one regressor) and then how adding relevant regressors to our model can improve the fit and limit omitted variable bias. We used a box-plot to show that residuals were correlated with a variable that was not yet included in our model. However, we did not discuss how we can prove to ourselves in a rigorous way that the goodness-of-fit in the second model is better than the one in the first.

\subsubsection*{Goodness-of-Fit ($R^2$)}

Let's start by returning to our definition of the sum of squared residuals. We can re-write it in terms of the residuals in the following way.

\begin{align}
SSR(\beta_0,...,\beta_k) &= \sum_i ( y_i -  (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} ) )^2 \\
 &= \sum_i ( y_i -  (\hat{y}_i ) )^2 \\
 &= \sum_i  ( \epsilon_i - 0  )^2 \\
 &= (n-1) Var(\epsilon) 
\end{align}

Written this way, we can see that the sum of squared residuals is proportional to the variation of the residuals. In other words, {\bf SSR is the variation in house prices that our model does not explain}. 

It turns out that we can also write down a formula for the variation in house prices that our model does explain, the { \bf{explained sum of squares (SSE)}}, as well as one for the total variation in house prices, or {\bf{total sum of squares (SST)}}. 

\begin{align}
SSR(\beta_0,...,\beta_k) &= \sum_i  ( \epsilon_i - 0  )^2 = (n-1) Var(\epsilon) \\
SSE(\beta_0,...,\beta_k) &= \sum_i  ( \hat{y}_i - \bar{y} )^2 = (n-1) Var(\hat{y}) \\
SST &= \sum_i  ( y_i - \bar{y} )^2 = (n-1) Var(y) 
\end{align}

Together, these components of variation have the following relationship:

\begin{align}
SST &= SSE + SSR
\end{align}

With this in mind, $R^2$ can be defined as either the share of total variation explained by the model,

\begin{align}
R^2 &= \frac{SSE}{SST} \\
&= \frac{Var(\hat{y})}{Var(y)} 
\end{align}

Or as 1 minus the share of total variation unexplain by the model,

\begin{align}
R^2 &= 1 - \frac{SSR}{SST} \\
&= 1 -\frac{Var(\epsilon)}{Var(y)} 
\end{align}

Let's calculate the R-squared for a model and compare it to the regression output of R.

<<compare_rsquared>>=
# fit the simple model
fit<-lm(log_sale_price ~ log_living_sqft,data=data.model)

# calculate R-squared
1-var(resid(fit))/var(data.model$log_sale_price)
# compare to the regression output
summary(fit)
@

The regression output refers to $R^2$ as "multiple R-squared", which is simply an allusion to $R^2$ under the multiple linear regression framework---it is still calculated as described above.

{\bf Adjusted R-squared}, however, is different from $R^2$. $R^2$ does not account for the number of regressors being added to the model. So for small datasets, $R^2$ will always increase slightly regardless of the explanatory power of the variable being added to the model. Adjusted R-squared takes the number of added regressors into account and makes the necessary adjustment.

\subsubsection*{Model Selection}

Many methods are available for comparing and selecting the best model (from a statistical perspective). 

For the following examples, let's consider two models. The first is called our { \bf restricted model}. It is our best model so far that will act as our base model. The second is called our {\bf unrestricted model}. This is our new model that we would like to test against the first. Let's estimate these models now from the house price data:

<<models_for_comparison>>=
fit.res<-lm(log_sale_price ~ log_living_sqft,data=data.model)
fit.unres<-lm(log_sale_price ~ log_living_sqft + factor(neighborhood),data=data.model)
@

\subsubsection*{Testing Adjusted R-squared ($\bar{R}^2$)}

Comparing the Adjusted R-squared of two model specifications is a popular method of choosing one model over another.

If $\bar{R}^2_{unres} > \bar{R}^2_{res}$, then we reject the hypothesis that the unrestricted model containing no explanatory power over the restricted one. Then the unstricted model becomes our new basis for comparison. 

Let's try this test on our house price models.

<<comparing_adjr2>>=
summary(fit.res)$adj.r.squared
summary(fit.unres)$adj.r.squared
@

In this example, we would reject the hypothesis that the model including the neighborhood dummies contains no explanatory power beyond that of the restricted model without the dummies.

\subsubsection*{F-test}

In general, an F-test is used to test to the equivalence of variances for two normal distributions. Since well-specified linear regression models will have residuals that are normally distributed (we have not dicussed this), the F-test is commonly used to test the equivalence of the unexplained variation between two models.

For this test, we calculate an F-statistic from the SSR of both models and compared it to the critical value from the F-distribution. For the number of regressors in each respective model ($p$), and the number of observations ($n$), the F-statistic is defined as:

\begin{align}
F &= \frac{SSR_{res} - SSR_{unres}}{p_{unres} - p_{res}} \bigg{/} \frac{SSR_{unres}}{n - p_{unres}}
\end{align}

This F-statistic has the distribution $F(p_{unres} - p_{res}, n - p_{unres})$. The critical value is then chosen from this distribution based on the desired significance level (usually .05).

Let's try this test on our house price models.

<<f_test>>=
anova(fit.res,fit.unres)
@

Again, we would reject the hypothesis that the model including the neighborhood dummies contains no explanatory power beyond that of the restricted model without the dummies.

However, notice that the F-test provides us with a degree of confidence for rejecting the restricted model. It also shows the number of regressor that were added in the unstricted model-- the neighborhood factor results in 54 dummies being added.

\subsubsection*{Other Methods for Model Selection}

Other methods exist for model selection, but will not be covered here. You can read more about them on wikipedia or in your favorite statistics / ecomometrics textbook. Some of the more popular ones are the Likelihood Ratio test (LR), Akaike information criterio (AIC), and the Bayesian informaiton criterion (BIC).

\subsection*{Generalized Linear Models (GLM)  }

For the oridinary least squares (OLS) regression framework above, we were able to describe the fitted values $\hat{y}_i$ as the expected value of $y_i$ given our regressors ($x_i$) and the estimated betas ($\hat{\beta}$):

\begin{align}
  \hat{y}_i = E(y_i|\hat{\beta};x_i) \label{ols_expectation}
\end{align}

The generalized linear model (GLM) framework expands upon OLS by allowing for the response variable ($y$) to be of a wider range of distributions. To do this, we allow for inclusion of a {\bf link function} that we will call $g$. However, in following the notation in Equation \ref{ols_expectation} above, we will reference its inverse, $\inv{g}$. 

\begin{align}
  \hat{y}_i = E(y_i|\inv{g},\hat{\beta};x_i)
\end{align}

The link function is that which connects $\hat{y}_i$ to $E(y_i|\beta;x_i)$. For example, if $g(z)=ln(z)$, then $\inv{g}(z)=e^z$ and

\begin{align}
  ln(\hat{y}_i) &= E(y_i|\hat{\beta};x_i) \\
  &= \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik} \\
  \hat{y}_i &= E(y_i|e^z,\beta;x_i) \\
  &= e^{\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik}}
\end{align}

Then our loss function (analogous to SSR but a more general) can be stated as:

\begin{align}
   \hat{\beta} = \underset{\beta}{argmin} \; ||y_i - E(y_i|\inv{g},\beta;x_i)||
\end{align}

It not important, in terms of this class, to know how this loss function is minimized. However, you should know that, unlike the SSR in OLS, this loss function does not always have a closed-form solution. So GLM models can take much longer to estimate than OLS models.

\subsubsection*{GLM Families}

With the {\bf glm()} function provided by R, we can estimate GLM models just like we did with {\bf lm()}, we only need to provide the family and link function so it knows the type of response we are trying to model. 

I will show examples of the gaussian (normal), logistic, and poisson regression. However, the R documentation for {\bf glm()} provides a full list of possible models and the wikipedia page for "generalized linear models" has a great overview table of the possible models.

\subsubsection*{Gaussian (identity)}

\begin{tabularx}{\textwidth}{r@{}l@{}}
  { Distribution of $y$ } &: Normal \\
  { Link Function } &: $g(z)=z$ \\
  { Inverse Function } &: $\inv{g}(z)=z$ \\
\end{tabularx}

\vspace{2ex}
\hspace*{\fill}%

Notice that because the link is the identity, the GLM gaussian model ends up taking the same form as the OLS regression.

\begin{align}
  \hat{y}_i &= E(y_i|\inv{g},\hat{\beta};x_i) \\
  &= E(y_i|z,\hat{\beta};x_i) \\
  &= \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik}
\end{align}

We can estimate GLM version of our house price model as follows:

<<glm_gaussian>>=
fit.gaussian<-glm(log_sale_price ~ log_living_sqft,
                  data=data.model,
                  family=gaussian(link="identity"))
summary(fit.gaussian)
@


\subsubsection*{Binomial (logit)}

\begin{tabularx}{\textwidth}{r@{}l@{}}
  { Distribution of $y$ } &: Binomial \\
  { Link Function } &: $g(z)=ln( z / (1-z))$ \\
  { Inverse Function } &: $\inv{g}(z)= exp(z) / (1+exp(z)) $ \\
\end{tabularx}

\begin{align}
  \hat{y}_i &= E(y_i|\inv{g},\hat{\beta};x_i) \\
  &= E(y_i| \frac{exp(z)}{1+exp(z)},\hat{\beta};x_i) \\
  &= \frac{exp(\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik})}{1+exp(\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik})}
\end{align}

To test this model, we need some data that contains responses that represent successes and failures. Classification type problems can be framed in these terms if we consider one class to be a success and all other classes to be failures.

R provides a toy dataset called "iris" that it famously used to introduce classification. It contains observations on the sepal width/length and petal width/length of 3 species of iris. 

Let's create a response variable that is 1 if the species is "setosa" and 0 otherwise, and then fit a model with one of the observed variables.

<<glm_logit>>=
data("iris")
iris<-iris %>% mutate(is.setosa=ifelse(Species=="setosa",1,0))
fit.logit<-glm(is.setosa ~ Sepal.Length, 
               data=iris,
               family=binomial(link="logit"))
summary(fit.logit)
@

The coefficients of the logistic regression can be interpretted as $\beta$ change in log-odds of a success with a 1 unit increase in the variable of interest.

What are log-odds? Well the odds of a success are defined as: 

$$odds = \frac{p(success)}{p(failure)}$$ 

So the log-odds is simply the odds on log-scale: 

\begin{align}
  log(odds) &= log\bigg(\frac{p(success)}{p(failure)}\bigg) \\
            &= log(p(success)) - log(p(failure))
\end{align}

For example, the probability of drawing a setosa from the dataset at random is $p=.33$. So the odds of drawing a setosa are $.33/.67=.5$ and the log-odds are $log(.33)-log(.67)=-0.69$

\subsubsection*{Poisson (log)}

\begin{tabularx}{\textwidth}{r@{}l@{}}
  { Distribution of $y$ } &: Poisson \\
  { Link Function } &: $g(z)=ln(z)$ \\
  { Inverse Function } &: $\inv{g}(z)= exp(z) $ \\
\end{tabularx}

\begin{align}
  \hat{y}_i &= E(y_i|\inv{g},\hat{\beta};x_i) \\
  &= E(y_i|exp(z),\hat{\beta};x_i) \\
  &= exp(\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \cdots + \hat{\beta}_1 x_{ik})
\end{align}

The Poisson allows us to estimate response variables that represent counts over a fix period of time. 

For example, I have some Uber logins for a specific geographic region. The original dataset is simply a vector of timestamps captured for 4 months. The data shown below have been aggregated to 15 minute intervals and show the cycles of logins for each day of the week.

<<p2_daily_cycle,echo=FALSE,fig.height=4,fig.width=8>>=
load("data/logins15.Rdata")
# daily cycle
logins15 %>% group_by(wday,h.m) %>% summarise(logs=mean(log)) %>%
  mutate(weekend=ifelse((wday=="Sun" | wday=="Sat"),"Weekend","Weekday") ) %>%
  ggplot(.,aes(h.m,logs,group=wday,colour=wday)) + geom_line() +
  scale_x_continuous(breaks=seq(0,23,4)) + 
  labs(y="Mean Logins",x="",colour="") + facet_grid(weekend~.)
@

Let's try to write down a model that captures the difference in behavior between the weekdays and weekends.

<<model_logins>>=
logins15<-logins15 %>% 
  mutate(split=ifelse((wday=="Sun" | wday=="Sat"),"Weekend","Weekday") )
fit.poisson<-glm(log ~ factor(split) + factor(h) + factor(split):factor(h) ,
                 data=logins15,
                 family=poisson(link="log"))
@

The model above takes into consideration the weekday/weekend split, the hour of the day, and the interaction of the two together. Now let's test the model and see how many logins it predicts for an average {\bf weekend at 4am} and for an average {\bf weekday at 7am}.

<<predict_logins>>=
predict(fit.poisson,
  newdata=data.frame(split=c("Weekend","Weekday"),h=c(4,7)),
  type="response")
@

\end{document}