\documentclass[10pt]{article}
\usepackage{amssymb, amsmath, amsthm, verbatim, layout, setspace, graphicx, float, geometry}
\usepackage{booktabs}

\newcommand{\textnote}[1]{\textbf{\textit{#1}}}
\newcommand{\code}[1]{\textbf{\textit{#1}}}
\newcommand{\grad}{\nabla}

% ------ layout ------

\begin{document}
\title{Linear Regression: Part 2}
\author{Jeremy Oldfather}
\maketitle

<<setup,echo=FALSE,message=FALSE>>=
opts_chunk$set(fig.align="center",size="small")
library(ggplot2)
library(readr)
library(dplyr)
@


<<catchup,warning=FALSE,message=FALSE,cache=TRUE,echo=FALSE>>=
# see: https://github.com/theoldfather/DC_Properties
sales<-read_csv("~/Projects/DC_Properties/data/sales_clean.csv")
features<-read_csv("~/Projects/DC_Properties/data/features_clean.csv")
details<-read_csv("~/Projects/DC_Properties/data/details_clean.csv")

d<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  select(sale_price,living_sqft) 

fit_line<-function(y,x){
  m <- (y[2]-y[1])/(x[2]-x[1])
  b <- y[1] - m*x[1]
  return(c(b,m))
}

fit_simple_lm<-function(y,x){
  n<-length(y)
  syx<-sum(y*x)
  sy<-sum(y)
  sx<-sum(x)
  sxx<-sum(x^2)
  b1<- (syx - sy*sx/n)/(sxx - sx^2/n)
  b0<- (sy - b1*sx)/n
  return(c(b0,b1))
}

# subset some data for a regression
# and make sure both sale_price and living_sqft are log-scaled
data.model<-sales %>% 
  filter(sale_price>0) %>% 
  left_join(features,by="ssl") %>%
  left_join(select(details,-sale_price,-neighborhood),by="ssl") %>%
  filter(!is.na(living_sqft) & living_sqft>0) %>%
  mutate(log_sale_price=log(sale_price),
         log_living_sqft=log(living_sqft))

fit<-lm(log_sale_price ~ log_living_sqft, data=data.model)

@


\subsection*{Multiple Linear Regression - Last Time }

When we regress $y$ on more than one independent variable, say ($x_1, x_2,...,x_k$) rather than simply ($x_1$), the regression framework is referred to as {\bf multiple linear regression}.

What might motivate us to introduce multiple regressors? One reason might be that we have reason to suspect {\bf omitted variable bias}. In order for our model to be subject to omitted variable bias, we must have both of the following problems:

\begin{description}
  \item [$E(y|x_1,x_2) \ne E(y|x_1) $] :  Meaning our expectation about the dependent variable $y$ changes when we include the additional independent variable. If $E(y|x_1,x_2) = E(y|x_1)$, we would say that $y$ is conditionally independent of $x_2$---meaning either $x_2$ is completely irrevelant to predicting $y$, or that any information contained in $x_2$ is already contained in $x_1$.
  \item [$Cor(x_1, x_2) \ne 0 $] :  Meaning the additional variable $x_2$ must be correlated with a current dependent variable $x_1$.
\end{description}

For example, when we plot the distribution of residuals from our previous model by neighborhood, we can see that the variable {\bf neighborhood} meets the first critera for omitted variable bias since for Anacostia we would consistently overstate the impact of a change in living area on the change in sales price.

<<boxplot_residuals,fig.height=3,dev='pdf'>>=
selected.nhoods<-c("FOGGY BOTTOM","ANACOSTIA","LEDROIT PARK","TAKOMA","GEORGETOWN")
data.frame(e=resid(fit),g=data.model$neighborhood) %>%
  filter(g %in% selected.nhoods) %>%
  ggplot(aes(y=e,x=g)) + geom_boxplot()
@

To check the second criteria, that $Cor(living\_area,neighborhood) \ne 0$, we can plug a dummy variable for each neighborhood into R's {\bf cor()} function.

<<dummy_cor>>=
cor(data.model$log_living_sqft,data.model$neighborhood=="ANACOSTIA")
cor(data.model$log_living_sqft,data.model$neighborhood=="FOGGY BOTTOM")
cor(data.model$log_living_sqft,data.model$neighborhood=="GEORGETOWN")
cor(data.model$log_living_sqft,data.model$neighborhood=="LEDROIT PARK")
cor(data.model$log_living_sqft,data.model$neighborhood=="TAKOMA")
@

Anacostia, Ledroit Park, and Takoma are either weakly or not at all correlated with living area, but Foggy Bottom is negatively correlated and Georgetown is weakly positively correlated.

To see the connection between including these neighborhoods in the regression, I will create a new variable that sets all other neighborhoods to "other". I put "aa" at the beginning because R treats the first element of a factor variable as the base value for the regression. So by doing this, we can see how the neighborhoods we analyzed above affect our regression in terms of all other DC neighborhoods.

<<fit_5_nhoods>>=
fit2<-data.model %>% 
  mutate(nhoods5=ifelse(neighborhood %in% selected.nhoods,
                        neighborhood,
                        "aa_other")
         ) %>%
  lm(log_sale_price ~ log_living_sqft + factor(nhoods5),data=.)
summary(fit2)
@

Comparing the coefficients for each neighborhood with the distribution of their residuals in the boxplot above, we can see the pattern. The coefficient for a dummy shifts our fitted line up or down by the amount of the coefficient.  For example, when all the dummies are turned off (equal zero), 

\begin{align*}
 \hat{y}_i &= 7.913192 + 0.699649 x_i
\end{align*}

But when Anacostia equals 1,

\begin{align*}
\hat{y}_i &= 7.913192 + 0.699649 x_i - 0.832172(1) \\
  &= 7.08102 + 0.699649 x_i
\end{align*}

Now I will add dummies for the rest of the DC neighborhoods and run the regression again.

<<fit_all_nhoods>>=
fit3<-lm(log_sale_price ~ log_living_sqft + factor(neighborhood),data=data.model)
coef(fit3)[1:2]
summary(resid(fit3))
@

\subsection*{Multiple Linear Regression - Continuation }


\newpage

% ----- start appendices -----



\end{document}