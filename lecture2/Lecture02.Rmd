---
title: "Data Input, Management and Output"
author: "Jeremy Oldfather"
date: "September 9, 2016"
output: html_document
---

## Lecture 2: Data Formats

What is data? How we answer this question dictates the formats we should learn to read, write, and analyze with R. 

### Structured

#### Delimited

The most widely used format for spreadsheet-type data is a text file where the columns of the spreadsheet are delimited by a specific character--usually a comma (`.csv`), tab (`.tsv` or `.txt`), or pipe **|** (`.pip` or `.txt`).

##### Reading / Writing

R has built-in functions to read and write delimited files: `read.csv()`, `read.table()`, `read.delim()`. However, there are better versions of these available in the package `readr`: `read_csv()`, `read_table()`, `read_delim()`. 
We can read local files from our computers or remotely from URLs. Let's read a csv of unemployment data from github.

```{r,warning=FALSE}
library(readr)
unemp<-read_csv("https://raw.githubusercontent.com/datasets/us-employment-bls/master/data/aat1.csv")
```

`readr` gives us feedback about how it parsed the csv. For example, it realized that the `Year` column only contained integer values and decided to store that column as an integer vector. This saves space in memory and removes the need for us to do this manually. 

The variable names in the csv, though very descriptive, are not friendly to type. So we might like to rename the variables then save the data and a codebook to disk for later use.

```r
# new names for the columns
short_names<-c('year',
               'civ_noninst_pop',
               'civ_lf_tot',
               'pct_pop',
               'emp_tot',
               'emp_pct_pop',
               'emp_ag_tot',
               'emp_nonag_tot',
               'unemp',
               'unemp_pct_lf',
               'not_in_lf',
               'footnotes')
               
# create a codebook with the new names, old names, and the type of each column
codebook<-data.frame(name=short_names,
                     descripition=names(unemp),
                     type=as.character(lapply(unemp,typeof)))
                     
# now rename the variables
names(unemp)<-short_names

# write both the codebook and rename data to disk as tab-delimited files
write_tsv(codebook,"lecture2/data/unemp_codebook.txt")
write_tsv(unemp,"lecture2/data/unemp.txt")
```

#### Proprietary Formats

##### Reading from Excel



### Semi-Structured

Semi-structured dataset are those do not cleanly fit in a spreadsheet format. These sources are now widely available across the internet. Examples include feeds from sites like Facebook, Twitter, and Google. Even city governments are beginning to provide data feeds of services like transportation, crime, construction, etc. 

#### JSON

WMATA is an example of a local service that provides a data feed. For detailed info, see [developer.wmata.com](https://developer.wmata.com).

One of the many things WMATA allows us to do is get informations on [bus positions](https://developer.wmata.com/docs/services/54763629281d83086473f231/operations/5476362a281d830c946a3d68). We saw above that the `read` class of functions allows us to get data from URLs. However, they do not allows us to customize the query, which is something we need to since most APIs require us to pass a key. For this, we can use the `httr` packages.

Let's find out where all the 70 buses are right now.

```{r}
library(httr)
resp<-GET("https://api.wmata.com/Bus.svc/json/jBusPositions",
          add_headers(api_key = "ea918d19ee864a81919bb7b1b98d9aab"),
          query=list(RouteID="70")
          )
```

The code above creates a response object that contains both the raw JSON response from WMATA and also a parsed version that `httr` automatically turned into a list. Let's the details of the first bus in the list object.

```{r}
# look at the first bus in the data feed
r70<-content(resp)$BusPositions
r70[[1]]
```

What about the raw JSON?

```{r}
content(resp,"text")
```

That is messy. We can clean it up with the `jsonlite` package.

```{r}
library(jsonlite)
toJSON(r70[[1]],pretty=T)
```

Ah, much better. Notice we just turned a list object into JSON text. JSON is a great way to export arbitrary list objects from R to other programs.

Let's save this "pretty" version of the data feed to a file for use later.

```r
write_file(toJSON(content(resp),pretty=T),"lecture2/data/route_70.json")
```

To load it again later, we can read the file into a character vector and then parse it from JSON to a list.

```r
r70<-fromJSON(read_file("lecture2/data/route_70.json"))
```

### Unstructured / Raw

Unstructured data like [text](http://blogs.lse.ac.uk/businessreview/2016/08/18/businesses-can-no-longer-ignore-social-media-sentiment-analysis/) and [images](http://www.wsj.com/articles/satellites-hedge-funds-eye-in-the-sky-1471207062) are becoming increasingly more important for economic and financial forecasting. We can deal with both of these in R.

#### Text

```{r}
hamlet<-read_file("http://www.gutenberg.org/cache/epub/2265/pg2265.txt")
length(hamlet)     # 1, everything is packed into a single character element
```
If we want individual words, we can split on spaces.

```{r}
words<-unlist(strsplit(hamlet," "))
length(words)
head(words)
```

Many of the words in the text are not unique. We can factor to save space.

```{r}
object.size(words)
tokens<-factor(words)
object.size(tokens)
```

That is a little smaller. Let make everything lowercase so that terms like "The" and "the" are the same token.

```{r}
words<-tolower(words)
tokens<-factor(words)
object.size(tokens)
```

What if we are only interested in "english" words? Let's load a dictionary and remove all the non-dictionary words from Hamlet.

```{r}
en_dict<-read_lines("https://raw.githubusercontent.com/dwyl/english-words/master/words.txt")
en_words <- words[words %in% en_dict]
length(en_words)  # 20223
```

How many of the remaining words have a negative connotation? Let's load a sentiment dictionary and find the share of negative words in Hamlet.

```{r}
neg_dict<-read_lines("http://ptrckprry.com/course/ssd/data/negative-words.txt",skip=35)
sum(en_words %in% neg_dict) / length(en_words)
```

We could repreat this process with another Shakespeare work and compare their sentiment. The accuracy of the estimate would be even better if we could convince a shakespearean literary expert to write a custom dictionary of negatives words for us. But our result is okay for a few minutes of effort.

#### Images

R can load JPEG images using the `jpeg` package.

```{r}
library(jpeg)
four<-readJPEG("data/four.jpeg")
dim(four)       # a 3rd order tensor
```

```{r}
image(four[,,1])
```

The orientation is strange. Take the transpose and then reverse it.

```{r}
image(t(four[,,1])[,500:1])
flip<-function(x) { t(x)[,ncol(x):1] }
```

That's a neat trick. Why is this useful? What if we had handwritten digits (a lot of them) and we wanted to decide was number was represented in each image without looking at each one? 

```{r}
digits<-read_csv("data/digits.csv")
names(digits)[1:5]
```
What might a typical 9 look like?

```{r}
label<-digits$label
images<-as.matrix(digits[,-1])
nines<-images[label==9,]
typical_nine<-matrix(colMeans(nines),ncol=28)
image(typical_nine[,28:1])
```

This is a toy of example of image recognition that is an area of machine learning. Hedge Funds have begun using image recognition on satellite imagery to make financial forecasts ([source](http://www.wsj.com/articles/satellites-hedge-funds-eye-in-the-sky-1471207062)). 
